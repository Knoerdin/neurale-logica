{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset ,DataLoader\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input / output format\n",
    "Input is in de vorm van twee vectors die waardes 1 en 0 bevatten (waar of onwaar).\\\n",
    "De waardes van de **eerste** vector moeten worden gelezen alsof er conjuncten tussen staan, een voorbeeld is de vector (0,1) die kan worden gelezen als: niet A en B.\\\n",
    "De waardes van de **tweede** vector moeten worden gelezen alsof er disjuncten tussen staan, verder zijn er altijd exact twee true values in de vector.\\\n",
    "Deze vectoren worden vergeleken (bijvoorbeeld (0,0,1) en (0,1,1) is True) en de output zal true of false zijn.\\\n",
    "De hoeveelheid mogelijke combinaties zijn 2^n * (n!/(2!(n-2)!)), voor n = 3 is dit 24.\\\n",
    "\\\n",
    "Alle combinaties waar de eerste vector gelijk is aan de tweede vector worden verwijderd.\\\n",
    "\\\n",
    "In deze poging ga ik de tien keer regel gebruiken, dit houd in dat voor elk mogelijk input-output paar er tien kopieÃ«ren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((0, 0, 0), (0, 1, 1)), False), (((0, 0, 0), (1, 0, 1)), False), (((0, 0, 0), (1, 1, 0)), False), (((0, 0, 1), (0, 1, 1)), True), (((0, 0, 1), (1, 0, 1)), True), (((0, 0, 1), (1, 1, 0)), False), (((0, 1, 0), (0, 1, 1)), True), (((0, 1, 0), (1, 0, 1)), False), (((0, 1, 0), (1, 1, 0)), True), (((0, 1, 1), (1, 0, 1)), True), (((0, 1, 1), (1, 1, 0)), True), (((1, 0, 0), (0, 1, 1)), False), (((1, 0, 0), (1, 0, 1)), True), (((1, 0, 0), (1, 1, 0)), True), (((1, 0, 1), (0, 1, 1)), True), (((1, 0, 1), (1, 1, 0)), True), (((1, 1, 0), (0, 1, 1)), True), (((1, 1, 0), (1, 0, 1)), True), (((1, 1, 1), (0, 1, 1)), True), (((1, 1, 1), (1, 0, 1)), True), (((1, 1, 1), (1, 1, 0)), True)]\n"
     ]
    }
   ],
   "source": [
    "#generate input vectors\n",
    "def generate_input_vectors(n):\n",
    "    fist_vec = list(it.product([0, 1], repeat=n))\n",
    "    second_vec = []\n",
    "    for i in fist_vec:\n",
    "        if i.count(1) == 2:\n",
    "            second_vec.append(i)\n",
    "    total_vec = list(it.product(fist_vec, second_vec))\n",
    "    for i in total_vec:\n",
    "        if i[0] == i[1]:\n",
    "            total_vec.remove(i)\n",
    "    return total_vec\n",
    "def generate_val(n):\n",
    "    fist_vec = list(it.product([0, 1], repeat=n))\n",
    "    second_vec = []\n",
    "    for i in fist_vec:\n",
    "        if i.count(1) == 2:\n",
    "            second_vec.append(i)\n",
    "    total_vec = list(it.product(fist_vec, second_vec))\n",
    "    return total_vec\n",
    "#generate true output\n",
    "def generate_true(x):\n",
    "    out = []\n",
    "    for pair in x:\n",
    "        for i in range(len(pair[0])):\n",
    "            if pair[0][i] == 1 and pair[1][i] == 1:\n",
    "                out.append(((pair[0],pair[1]),True))\n",
    "                break\n",
    "            elif i == len(pair[0]) - 1:\n",
    "                out.append(((pair[0],pair[1]),False))\n",
    "    return out\n",
    "print(generate_true(generate_input_vectors(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = [torch.tensor(i[0]) for i in data]\n",
    "        self.y = torch.tensor([i[1] for i in data], dtype=torch.bool)\n",
    "        self.data = list(zip(self.X, self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = generate_true(generate_input_vectors(5))\n",
    "dataset_train = CustomDataset(data_train)\n",
    "data_val = generate_true(generate_val(5))\n",
    "dataset_val = CustomDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 10)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/500], Loss: 0.9158954620361328\n",
      "Epoch: [2/500], Loss: 0.769905686378479\n",
      "Epoch: [3/500], Loss: 0.24967069923877716\n",
      "Epoch: [4/500], Loss: 0.2581193447113037\n",
      "Epoch: [5/500], Loss: 0.7415211200714111\n",
      "Epoch: [6/500], Loss: 0.25861889123916626\n",
      "Epoch: [7/500], Loss: 0.22418934106826782\n",
      "Epoch: [8/500], Loss: 0.2063371241092682\n",
      "Epoch: [9/500], Loss: 0.7607000470161438\n",
      "Epoch: [10/500], Loss: 0.18082782626152039\n",
      "Epoch: [11/500], Loss: 0.18745160102844238\n",
      "Epoch: [12/500], Loss: 0.13137900829315186\n",
      "Epoch: [13/500], Loss: 0.1553906947374344\n",
      "Epoch: [14/500], Loss: 0.5288422107696533\n",
      "Epoch: [15/500], Loss: 0.4234210252761841\n",
      "Epoch: [16/500], Loss: 0.9661648869514465\n",
      "Epoch: [17/500], Loss: 0.6987106800079346\n",
      "Epoch: [18/500], Loss: 0.4150815010070801\n",
      "Epoch: [19/500], Loss: 0.33340224623680115\n",
      "Epoch: [20/500], Loss: 0.8342223763465881\n",
      "Epoch: [21/500], Loss: 0.031053654849529266\n",
      "Epoch: [22/500], Loss: 0.6232700347900391\n",
      "Epoch: [23/500], Loss: 0.2265084981918335\n",
      "Epoch: [24/500], Loss: 0.8287472128868103\n",
      "Epoch: [25/500], Loss: 0.6346302032470703\n",
      "Epoch: [26/500], Loss: 0.19591179490089417\n",
      "Epoch: [27/500], Loss: 0.5005765557289124\n",
      "Epoch: [28/500], Loss: 0.08922279626131058\n",
      "Epoch: [29/500], Loss: 0.2594977617263794\n",
      "Epoch: [30/500], Loss: 0.16285476088523865\n",
      "Epoch: [31/500], Loss: 0.19128680229187012\n",
      "Epoch: [32/500], Loss: 0.07212093472480774\n",
      "Epoch: [33/500], Loss: 0.26511305570602417\n",
      "Epoch: [34/500], Loss: 0.11110924929380417\n",
      "Epoch: [35/500], Loss: 0.02340301126241684\n",
      "Epoch: [36/500], Loss: 1.0150022506713867\n",
      "Epoch: [37/500], Loss: 0.18941403925418854\n",
      "Epoch: [38/500], Loss: 0.15218757092952728\n",
      "Epoch: [39/500], Loss: 0.2815791964530945\n",
      "Epoch: [40/500], Loss: 0.15685616433620453\n",
      "Epoch: [41/500], Loss: 0.17071522772312164\n",
      "Epoch: [42/500], Loss: 0.11448334902524948\n",
      "Epoch: [43/500], Loss: 0.37593650817871094\n",
      "Epoch: [44/500], Loss: 0.2683928906917572\n",
      "Epoch: [45/500], Loss: 0.2448074370622635\n",
      "Epoch: [46/500], Loss: 0.1170877069234848\n",
      "Epoch: [47/500], Loss: 0.1557026207447052\n",
      "Epoch: [48/500], Loss: 0.6341322660446167\n",
      "Epoch: [49/500], Loss: 0.41445574164390564\n",
      "Epoch: [50/500], Loss: 0.5023051500320435\n",
      "Epoch: [51/500], Loss: 0.12511664628982544\n",
      "Epoch: [52/500], Loss: 0.5874097943305969\n",
      "Epoch: [53/500], Loss: 0.18621952831745148\n",
      "Epoch: [54/500], Loss: 0.27308911085128784\n",
      "Epoch: [55/500], Loss: 0.43181464076042175\n",
      "Epoch: [56/500], Loss: 0.5046086311340332\n",
      "Epoch: [57/500], Loss: 0.40545541048049927\n",
      "Epoch: [58/500], Loss: 0.4215761721134186\n",
      "Epoch: [59/500], Loss: 0.009138322435319424\n",
      "Epoch: [60/500], Loss: 0.035067178308963776\n",
      "Epoch: [61/500], Loss: 0.07174540311098099\n",
      "Epoch: [62/500], Loss: 0.016960740089416504\n",
      "Epoch: [63/500], Loss: 0.2542646825313568\n",
      "Epoch: [64/500], Loss: 0.22905975580215454\n",
      "Epoch: [65/500], Loss: 0.0781932920217514\n",
      "Epoch: [66/500], Loss: 0.07062096893787384\n",
      "Epoch: [67/500], Loss: 0.021437112241983414\n",
      "Epoch: [68/500], Loss: 0.05677921697497368\n",
      "Epoch: [69/500], Loss: 0.20858663320541382\n",
      "Epoch: [70/500], Loss: 0.03453153744339943\n",
      "Epoch: [71/500], Loss: 0.05399511754512787\n",
      "Epoch: [72/500], Loss: 0.4234470725059509\n",
      "Epoch: [73/500], Loss: 0.22554796934127808\n",
      "Epoch: [74/500], Loss: 0.05985506251454353\n",
      "Epoch: [75/500], Loss: 0.1808159500360489\n",
      "Epoch: [76/500], Loss: 0.00409572571516037\n",
      "Epoch: [77/500], Loss: 0.02646932192146778\n",
      "Epoch: [78/500], Loss: 0.18069304525852203\n",
      "Epoch: [79/500], Loss: 0.01478250976651907\n",
      "Epoch: [80/500], Loss: 0.32646822929382324\n",
      "Epoch: [81/500], Loss: 0.2677387297153473\n",
      "Epoch: [82/500], Loss: 0.2725353538990021\n",
      "Epoch: [83/500], Loss: 0.16950973868370056\n",
      "Epoch: [84/500], Loss: 0.33630311489105225\n",
      "Epoch: [85/500], Loss: 0.023598765954375267\n",
      "Epoch: [86/500], Loss: 0.32082998752593994\n",
      "Epoch: [87/500], Loss: 0.30881816148757935\n",
      "Epoch: [88/500], Loss: 0.11879441887140274\n",
      "Epoch: [89/500], Loss: 0.18689893186092377\n",
      "Epoch: [90/500], Loss: 0.12846623361110687\n",
      "Epoch: [91/500], Loss: 0.18877525627613068\n",
      "Epoch: [92/500], Loss: 0.02513226866722107\n",
      "Epoch: [93/500], Loss: 0.007978052832186222\n",
      "Epoch: [94/500], Loss: 0.2077893614768982\n",
      "Epoch: [95/500], Loss: 0.023925328627228737\n",
      "Epoch: [96/500], Loss: 0.3232177495956421\n",
      "Epoch: [97/500], Loss: 0.16427457332611084\n",
      "Epoch: [98/500], Loss: 0.011606018990278244\n",
      "Epoch: [99/500], Loss: 0.023746361956000328\n",
      "Epoch: [100/500], Loss: 0.0015156927984207869\n",
      "Epoch: [101/500], Loss: 0.16550752520561218\n",
      "Epoch: [102/500], Loss: 0.0961078628897667\n",
      "Epoch: [103/500], Loss: 0.019845176488161087\n",
      "Epoch: [104/500], Loss: 0.05225872993469238\n",
      "Epoch: [105/500], Loss: 0.2074209302663803\n",
      "Epoch: [106/500], Loss: 0.025373676791787148\n",
      "Epoch: [107/500], Loss: 0.15868180990219116\n",
      "Epoch: [108/500], Loss: 0.0007613991037942469\n",
      "Epoch: [109/500], Loss: 0.2535645365715027\n",
      "Epoch: [110/500], Loss: 0.6285947561264038\n",
      "Epoch: [111/500], Loss: 0.6189180016517639\n",
      "Epoch: [112/500], Loss: 0.007830198854207993\n",
      "Epoch: [113/500], Loss: 0.005131995305418968\n",
      "Epoch: [114/500], Loss: 0.0480017215013504\n",
      "Epoch: [115/500], Loss: 0.005875088274478912\n",
      "Epoch: [116/500], Loss: 0.053611960262060165\n",
      "Epoch: [117/500], Loss: 0.04028991609811783\n",
      "Epoch: [118/500], Loss: 0.00856226310133934\n",
      "Epoch: [119/500], Loss: 0.04298119992017746\n",
      "Epoch: [120/500], Loss: 0.00023022349341772497\n",
      "Epoch: [121/500], Loss: 0.11743061244487762\n",
      "Epoch: [122/500], Loss: 0.00423846673220396\n",
      "Epoch: [123/500], Loss: 0.016669999808073044\n",
      "Epoch: [124/500], Loss: 0.11784538626670837\n",
      "Epoch: [125/500], Loss: 0.012277534231543541\n",
      "Epoch: [126/500], Loss: 0.06609106063842773\n",
      "Epoch: [127/500], Loss: 0.07766164839267731\n",
      "Epoch: [128/500], Loss: 0.000593772972933948\n",
      "Epoch: [129/500], Loss: 0.003477758262306452\n",
      "Epoch: [130/500], Loss: 0.00238944124430418\n",
      "Epoch: [131/500], Loss: 0.0778965875506401\n",
      "Epoch: [132/500], Loss: 0.18520832061767578\n",
      "Epoch: [133/500], Loss: 0.0026951958425343037\n",
      "Epoch: [134/500], Loss: 0.06260842084884644\n",
      "Epoch: [135/500], Loss: 0.16525065898895264\n",
      "Epoch: [136/500], Loss: 0.0027592263650149107\n",
      "Epoch: [137/500], Loss: 0.05929535627365112\n",
      "Epoch: [138/500], Loss: 0.26730042695999146\n",
      "Epoch: [139/500], Loss: 5.680035610566847e-05\n",
      "Epoch: [140/500], Loss: 0.11073905974626541\n",
      "Epoch: [141/500], Loss: 0.06567081063985825\n",
      "Epoch: [142/500], Loss: 0.10229668766260147\n",
      "Epoch: [143/500], Loss: 0.0349937304854393\n",
      "Epoch: [144/500], Loss: 3.7072753912070766e-05\n",
      "Epoch: [145/500], Loss: 0.0031215259805321693\n",
      "Epoch: [146/500], Loss: 0.13814455270767212\n",
      "Epoch: [147/500], Loss: 0.13211950659751892\n",
      "Epoch: [148/500], Loss: 0.002560690976679325\n",
      "Epoch: [149/500], Loss: 0.007999040186405182\n",
      "Epoch: [150/500], Loss: 0.2339412271976471\n",
      "Epoch: [151/500], Loss: 0.023441962897777557\n",
      "Epoch: [152/500], Loss: 0.10138750076293945\n",
      "Epoch: [153/500], Loss: 0.211755633354187\n",
      "Epoch: [154/500], Loss: 0.2276463508605957\n",
      "Epoch: [155/500], Loss: 0.00478624552488327\n",
      "Epoch: [156/500], Loss: 0.09499023854732513\n",
      "Epoch: [157/500], Loss: 0.17113278806209564\n",
      "Epoch: [158/500], Loss: 0.18574325740337372\n",
      "Epoch: [159/500], Loss: 0.11637008190155029\n",
      "Epoch: [160/500], Loss: 0.0006904641049914062\n",
      "Epoch: [161/500], Loss: 0.30178582668304443\n",
      "Epoch: [162/500], Loss: 0.1366278976202011\n",
      "Epoch: [163/500], Loss: 3.164907684549689e-05\n",
      "Epoch: [164/500], Loss: 0.0011893401388078928\n",
      "Epoch: [165/500], Loss: 0.0006259943475015461\n",
      "Epoch: [166/500], Loss: 0.0021670651622116566\n",
      "Epoch: [167/500], Loss: 0.001975632505491376\n",
      "Epoch: [168/500], Loss: 0.0010395569261163473\n",
      "Epoch: [169/500], Loss: 0.06664003431797028\n",
      "Epoch: [170/500], Loss: 0.12241856008768082\n",
      "Epoch: [171/500], Loss: 0.08013695478439331\n",
      "Epoch: [172/500], Loss: 0.002221764298155904\n",
      "Epoch: [173/500], Loss: 0.1102081686258316\n",
      "Epoch: [174/500], Loss: 0.0017489829333499074\n",
      "Epoch: [175/500], Loss: 0.10312674939632416\n",
      "Epoch: [176/500], Loss: 0.12698794901371002\n",
      "Epoch: [177/500], Loss: 0.018837254494428635\n",
      "Epoch: [178/500], Loss: 0.0014930760953575373\n",
      "Epoch: [179/500], Loss: 0.000842992914840579\n",
      "Epoch: [180/500], Loss: 0.05267701670527458\n",
      "Epoch: [181/500], Loss: 0.07341659069061279\n",
      "Epoch: [182/500], Loss: 0.04583481326699257\n",
      "Epoch: [183/500], Loss: 0.06773027032613754\n",
      "Epoch: [184/500], Loss: 0.034310102462768555\n",
      "Epoch: [185/500], Loss: 0.02482890896499157\n",
      "Epoch: [186/500], Loss: 0.048827387392520905\n",
      "Epoch: [187/500], Loss: 0.016949521377682686\n",
      "Epoch: [188/500], Loss: 0.0004046717076562345\n",
      "Epoch: [189/500], Loss: 0.11727946996688843\n",
      "Epoch: [190/500], Loss: 0.0011246104259043932\n",
      "Epoch: [191/500], Loss: 0.030351221561431885\n",
      "Epoch: [192/500], Loss: 0.01571536250412464\n",
      "Epoch: [193/500], Loss: 0.09533423185348511\n",
      "Epoch: [194/500], Loss: 0.0012458378914743662\n",
      "Epoch: [195/500], Loss: 0.05927293002605438\n",
      "Epoch: [196/500], Loss: 0.0444001741707325\n",
      "Epoch: [197/500], Loss: 0.022191446274518967\n",
      "Epoch: [198/500], Loss: 0.0009326579747721553\n",
      "Epoch: [199/500], Loss: 0.09881628304719925\n",
      "Epoch: [200/500], Loss: 0.04444558545947075\n",
      "Epoch: [201/500], Loss: 0.09252888709306717\n",
      "Epoch: [202/500], Loss: 0.10125380754470825\n",
      "Epoch: [203/500], Loss: 0.1073325052857399\n",
      "Epoch: [204/500], Loss: 0.04829300194978714\n",
      "Epoch: [205/500], Loss: 0.10097452253103256\n",
      "Epoch: [206/500], Loss: 0.00029811315471306443\n",
      "Epoch: [207/500], Loss: 0.0002405660634394735\n",
      "Epoch: [208/500], Loss: 0.021805034950375557\n",
      "Epoch: [209/500], Loss: 0.053258709609508514\n",
      "Epoch: [210/500], Loss: 0.02829005755484104\n",
      "Epoch: [211/500], Loss: 0.0002685068466234952\n",
      "Epoch: [212/500], Loss: 0.00025522164651192725\n",
      "Epoch: [213/500], Loss: 0.03402671590447426\n",
      "Epoch: [214/500], Loss: 0.0002644560590852052\n",
      "Epoch: [215/500], Loss: 0.00015870224160607904\n",
      "Epoch: [216/500], Loss: 0.07235079258680344\n",
      "Epoch: [217/500], Loss: 0.0003253161848988384\n",
      "Epoch: [218/500], Loss: 0.05043512582778931\n",
      "Epoch: [219/500], Loss: 0.05546528100967407\n",
      "Epoch: [220/500], Loss: 0.06190318614244461\n",
      "Epoch: [221/500], Loss: 0.05911603569984436\n",
      "Epoch: [222/500], Loss: 0.045928601175546646\n",
      "Epoch: [223/500], Loss: 0.03151737153530121\n",
      "Epoch: [224/500], Loss: 0.010264672338962555\n",
      "Epoch: [225/500], Loss: 0.00046384669258259237\n",
      "Epoch: [226/500], Loss: 0.00039349187863990664\n",
      "Epoch: [227/500], Loss: 0.08654226362705231\n",
      "Epoch: [228/500], Loss: 0.0250807236880064\n",
      "Epoch: [229/500], Loss: 0.029115311801433563\n",
      "Epoch: [230/500], Loss: 0.025808462873101234\n",
      "Epoch: [231/500], Loss: 0.046620335429906845\n",
      "Epoch: [232/500], Loss: 0.009048174135386944\n",
      "Epoch: [233/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [234/500], Loss: 0.021575521677732468\n",
      "Epoch: [235/500], Loss: 0.008772102184593678\n",
      "Epoch: [236/500], Loss: 6.454766116803512e-05\n",
      "Epoch: [237/500], Loss: 0.03313613682985306\n",
      "Epoch: [238/500], Loss: 0.036328136920928955\n",
      "Epoch: [239/500], Loss: 0.010707948356866837\n",
      "Epoch: [240/500], Loss: 0.02272542379796505\n",
      "Epoch: [241/500], Loss: 0.00020316598238423467\n",
      "Epoch: [242/500], Loss: 0.13938380777835846\n",
      "Epoch: [243/500], Loss: 0.029155777767300606\n",
      "Epoch: [244/500], Loss: 0.020201968029141426\n",
      "Epoch: [245/500], Loss: 0.02469392493367195\n",
      "Epoch: [246/500], Loss: 0.04852572828531265\n",
      "Epoch: [247/500], Loss: 0.0287003293633461\n",
      "Epoch: [248/500], Loss: 0.00015184957010205835\n",
      "Epoch: [249/500], Loss: 0.023048287257552147\n",
      "Epoch: [250/500], Loss: 0.04410836845636368\n",
      "Epoch: [251/500], Loss: 0.12781590223312378\n",
      "Epoch: [252/500], Loss: 6.085295899538323e-05\n",
      "Epoch: [253/500], Loss: 0.025926940143108368\n",
      "Epoch: [254/500], Loss: 0.03595516085624695\n",
      "Epoch: [255/500], Loss: 0.03218363970518112\n",
      "Epoch: [256/500], Loss: 0.01546449400484562\n",
      "Epoch: [257/500], Loss: 0.04274971783161163\n",
      "Epoch: [258/500], Loss: 0.017762785777449608\n",
      "Epoch: [259/500], Loss: 8.326421084348112e-05\n",
      "Epoch: [260/500], Loss: 0.024517247453331947\n",
      "Epoch: [261/500], Loss: 0.025749649852514267\n",
      "Epoch: [262/500], Loss: 3.385429226909764e-05\n",
      "Epoch: [263/500], Loss: 0.020537419244647026\n",
      "Epoch: [264/500], Loss: 0.03870965540409088\n",
      "Epoch: [265/500], Loss: 0.0\n",
      "Epoch: [266/500], Loss: 0.015380983240902424\n",
      "Epoch: [267/500], Loss: 5.5728789448039606e-05\n",
      "Epoch: [268/500], Loss: 0.0\n",
      "Epoch: [269/500], Loss: 0.0396527498960495\n",
      "Epoch: [270/500], Loss: 0.015460615046322346\n",
      "Epoch: [271/500], Loss: 0.025750495493412018\n",
      "Epoch: [272/500], Loss: 0.020582571625709534\n",
      "Epoch: [273/500], Loss: 0.022751348093152046\n",
      "Epoch: [274/500], Loss: 0.013064843602478504\n",
      "Epoch: [275/500], Loss: 0.01205859798938036\n",
      "Epoch: [276/500], Loss: 1.7344651496387087e-05\n",
      "Epoch: [277/500], Loss: 0.021719099953770638\n",
      "Epoch: [278/500], Loss: 0.030131850391626358\n",
      "Epoch: [279/500], Loss: 0.004287345800548792\n",
      "Epoch: [280/500], Loss: 0.02379513531923294\n",
      "Epoch: [281/500], Loss: 0.01191218476742506\n",
      "Epoch: [282/500], Loss: 0.0\n",
      "Epoch: [283/500], Loss: 0.01909896731376648\n",
      "Epoch: [284/500], Loss: 0.07714878022670746\n",
      "Epoch: [285/500], Loss: 0.005422692280262709\n",
      "Epoch: [286/500], Loss: 0.04348364472389221\n",
      "Epoch: [287/500], Loss: 0.01506737433373928\n",
      "Epoch: [288/500], Loss: 0.0\n",
      "Epoch: [289/500], Loss: 2.2411084501072764e-05\n",
      "Epoch: [290/500], Loss: 5.602805231319508e-06\n",
      "Epoch: [291/500], Loss: 0.009871070273220539\n",
      "Epoch: [292/500], Loss: 0.015121602453291416\n",
      "Epoch: [293/500], Loss: 0.028635751456022263\n",
      "Epoch: [294/500], Loss: 0.018773149698972702\n",
      "Epoch: [295/500], Loss: 2.0086499716853723e-05\n",
      "Epoch: [296/500], Loss: 0.0\n",
      "Epoch: [297/500], Loss: 0.0\n",
      "Epoch: [298/500], Loss: 0.01765892654657364\n",
      "Epoch: [299/500], Loss: 0.007675080560147762\n",
      "Epoch: [300/500], Loss: 0.015090391971170902\n",
      "Epoch: [301/500], Loss: 0.008445395156741142\n",
      "Epoch: [302/500], Loss: 0.018772119656205177\n",
      "Epoch: [303/500], Loss: 1.5616269593010657e-05\n",
      "Epoch: [304/500], Loss: 0.00800784770399332\n",
      "Epoch: [305/500], Loss: 0.005538490600883961\n",
      "Epoch: [306/500], Loss: 0.0\n",
      "Epoch: [307/500], Loss: 3.6358701436256524e-06\n",
      "Epoch: [308/500], Loss: 1.549718376736564e-06\n",
      "Epoch: [309/500], Loss: 0.0\n",
      "Epoch: [310/500], Loss: 0.004801193717867136\n",
      "Epoch: [311/500], Loss: 0.007718442007899284\n",
      "Epoch: [312/500], Loss: 0.007875263690948486\n",
      "Epoch: [313/500], Loss: 0.004323902539908886\n",
      "Epoch: [314/500], Loss: 0.011236611753702164\n",
      "Epoch: [315/500], Loss: 0.010187947191298008\n",
      "Epoch: [316/500], Loss: 0.020248599350452423\n",
      "Epoch: [317/500], Loss: 9.477089406573214e-06\n",
      "Epoch: [318/500], Loss: 0.005975199863314629\n",
      "Epoch: [319/500], Loss: 0.005981171503663063\n",
      "Epoch: [320/500], Loss: 7.748597568024707e-07\n",
      "Epoch: [321/500], Loss: 0.017578357830643654\n",
      "Epoch: [322/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [323/500], Loss: 0.009721001610159874\n",
      "Epoch: [324/500], Loss: 0.009787049144506454\n",
      "Epoch: [325/500], Loss: 0.01281499769538641\n",
      "Epoch: [326/500], Loss: 0.0033416030928492546\n",
      "Epoch: [327/500], Loss: 0.00898747704923153\n",
      "Epoch: [328/500], Loss: 0.006678762845695019\n",
      "Epoch: [329/500], Loss: 2.562995632615639e-06\n",
      "Epoch: [330/500], Loss: 0.0\n",
      "Epoch: [331/500], Loss: 5.1259844440210145e-06\n",
      "Epoch: [332/500], Loss: 0.011695045977830887\n",
      "Epoch: [333/500], Loss: 0.0\n",
      "Epoch: [334/500], Loss: 0.004616755060851574\n",
      "Epoch: [335/500], Loss: 0.038539864122867584\n",
      "Epoch: [336/500], Loss: 0.011487366631627083\n",
      "Epoch: [337/500], Loss: 0.011638723313808441\n",
      "Epoch: [338/500], Loss: 0.0042313276790082455\n",
      "Epoch: [339/500], Loss: 0.00961593259125948\n",
      "Epoch: [340/500], Loss: 1.668928234721534e-06\n",
      "Epoch: [341/500], Loss: 0.004735858645290136\n",
      "Epoch: [342/500], Loss: 0.0073282551020383835\n",
      "Epoch: [343/500], Loss: 0.01147235743701458\n",
      "Epoch: [344/500], Loss: 7.152552257139178e-07\n",
      "Epoch: [345/500], Loss: 0.0058994172140955925\n",
      "Epoch: [346/500], Loss: 0.008965205401182175\n",
      "Epoch: [347/500], Loss: 0.0027298820205032825\n",
      "Epoch: [348/500], Loss: 0.004645278677344322\n",
      "Epoch: [349/500], Loss: 1.311300479756028e-06\n",
      "Epoch: [350/500], Loss: 0.0021869344636797905\n",
      "Epoch: [351/500], Loss: 7.74860041019565e-07\n",
      "Epoch: [352/500], Loss: 1.311300479756028e-06\n",
      "Epoch: [353/500], Loss: 0.0036406568251550198\n",
      "Epoch: [354/500], Loss: 1.490114755142713e-06\n",
      "Epoch: [355/500], Loss: 1.251695948667475e-06\n",
      "Epoch: [356/500], Loss: 0.004218790214508772\n",
      "Epoch: [357/500], Loss: 0.0334441252052784\n",
      "Epoch: [358/500], Loss: 0.0018933331593871117\n",
      "Epoch: [359/500], Loss: 0.001779726822860539\n",
      "Epoch: [360/500], Loss: 0.017355049028992653\n",
      "Epoch: [361/500], Loss: 0.0016855786088854074\n",
      "Epoch: [362/500], Loss: 0.0\n",
      "Epoch: [363/500], Loss: 0.004071718081831932\n",
      "Epoch: [364/500], Loss: 0.01269847434014082\n",
      "Epoch: [365/500], Loss: 0.0037688075099140406\n",
      "Epoch: [366/500], Loss: 0.0038206528406590223\n",
      "Epoch: [367/500], Loss: 0.003477314952760935\n",
      "Epoch: [368/500], Loss: 0.001862984849140048\n",
      "Epoch: [369/500], Loss: 2.9802313861182483e-07\n",
      "Epoch: [370/500], Loss: 0.0018331143073737621\n",
      "Epoch: [371/500], Loss: 0.005445051938295364\n",
      "Epoch: [372/500], Loss: 0.004630813840776682\n",
      "Epoch: [373/500], Loss: 0.02256857417523861\n",
      "Epoch: [374/500], Loss: 0.0019230181351304054\n",
      "Epoch: [375/500], Loss: 0.0\n",
      "Epoch: [376/500], Loss: 0.003874983172863722\n",
      "Epoch: [377/500], Loss: 3.5762775496550603e-07\n",
      "Epoch: [378/500], Loss: 0.003941517323255539\n",
      "Epoch: [379/500], Loss: 0.0015591491246595979\n",
      "Epoch: [380/500], Loss: 3.576278118089249e-07\n",
      "Epoch: [381/500], Loss: 0.003969724755734205\n",
      "Epoch: [382/500], Loss: 0.0012849733466282487\n",
      "Epoch: [383/500], Loss: 0.006772096734493971\n",
      "Epoch: [384/500], Loss: 1.7881390590446244e-07\n",
      "Epoch: [385/500], Loss: 0.0016694184159860015\n",
      "Epoch: [386/500], Loss: 1.1920927533992653e-07\n",
      "Epoch: [387/500], Loss: 0.003283366095274687\n",
      "Epoch: [388/500], Loss: 0.0035947372671216726\n",
      "Epoch: [389/500], Loss: 1.7881390590446244e-07\n",
      "Epoch: [390/500], Loss: 0.0010711988434195518\n",
      "Epoch: [391/500], Loss: 1.1920927533992653e-07\n",
      "Epoch: [392/500], Loss: 0.0023629774805158377\n",
      "Epoch: [393/500], Loss: 0.009422138333320618\n",
      "Epoch: [394/500], Loss: 0.002110313158482313\n",
      "Epoch: [395/500], Loss: 0.0017462242394685745\n",
      "Epoch: [396/500], Loss: 1.7881390590446244e-07\n",
      "Epoch: [397/500], Loss: 1.7881390590446244e-07\n",
      "Epoch: [398/500], Loss: 0.003529696725308895\n",
      "Epoch: [399/500], Loss: 0.0009813804645091295\n",
      "Epoch: [400/500], Loss: 0.0026611078064888716\n",
      "Epoch: [401/500], Loss: 0.004576073959469795\n",
      "Epoch: [402/500], Loss: 0.0019931455608457327\n",
      "Epoch: [403/500], Loss: 0.0028837453573942184\n",
      "Epoch: [404/500], Loss: 0.0\n",
      "Epoch: [405/500], Loss: 0.0050197564996778965\n",
      "Epoch: [406/500], Loss: 0.00365456216968596\n",
      "Epoch: [407/500], Loss: 0.0023085151333361864\n",
      "Epoch: [408/500], Loss: 1.7881392011531716e-07\n",
      "Epoch: [409/500], Loss: 0.0038924310356378555\n",
      "Epoch: [410/500], Loss: 0.0\n",
      "Epoch: [411/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [412/500], Loss: 0.00216117687523365\n",
      "Epoch: [413/500], Loss: 0.0022687027230858803\n",
      "Epoch: [414/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [415/500], Loss: 0.0013297977857291698\n",
      "Epoch: [416/500], Loss: 0.0020078583620488644\n",
      "Epoch: [417/500], Loss: 0.0\n",
      "Epoch: [418/500], Loss: 0.004009421914815903\n",
      "Epoch: [419/500], Loss: 0.002429415239021182\n",
      "Epoch: [420/500], Loss: 0.0008481695549562573\n",
      "Epoch: [421/500], Loss: 0.0009167725220322609\n",
      "Epoch: [422/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [423/500], Loss: 0.0013712896034121513\n",
      "Epoch: [424/500], Loss: 0.001288064755499363\n",
      "Epoch: [425/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [426/500], Loss: 0.0\n",
      "Epoch: [427/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [428/500], Loss: 0.007596898823976517\n",
      "Epoch: [429/500], Loss: 5.9604641222676946e-08\n",
      "Epoch: [430/500], Loss: 0.0\n",
      "Epoch: [431/500], Loss: 0.0\n",
      "Epoch: [432/500], Loss: 0.0024519076105207205\n",
      "Epoch: [433/500], Loss: 0.0009985720971599221\n",
      "Epoch: [434/500], Loss: 0.0014512318884953856\n",
      "Epoch: [435/500], Loss: 0.0\n",
      "Epoch: [436/500], Loss: 0.0022148238494992256\n",
      "Epoch: [437/500], Loss: 0.0\n",
      "Epoch: [438/500], Loss: 0.0008062772685661912\n",
      "Epoch: [439/500], Loss: 0.008362365886569023\n",
      "Epoch: [440/500], Loss: 0.0015699033392593265\n",
      "Epoch: [441/500], Loss: 0.0\n",
      "Epoch: [442/500], Loss: 0.0\n",
      "Epoch: [443/500], Loss: 0.0009852470830082893\n",
      "Epoch: [444/500], Loss: 0.0012962095206603408\n",
      "Epoch: [445/500], Loss: 0.001518563716672361\n",
      "Epoch: [446/500], Loss: 0.0016205841675400734\n",
      "Epoch: [447/500], Loss: 0.0011428636498749256\n",
      "Epoch: [448/500], Loss: 0.0\n",
      "Epoch: [449/500], Loss: 0.001409627846442163\n",
      "Epoch: [450/500], Loss: 0.0007934827590361238\n",
      "Epoch: [451/500], Loss: 0.0018473031232133508\n",
      "Epoch: [452/500], Loss: 0.0010609687305986881\n",
      "Epoch: [453/500], Loss: 0.0012945448979735374\n",
      "Epoch: [454/500], Loss: 0.0013712302315980196\n",
      "Epoch: [455/500], Loss: 0.0013889434048905969\n",
      "Epoch: [456/500], Loss: 0.0\n",
      "Epoch: [457/500], Loss: 0.0004494553722906858\n",
      "Epoch: [458/500], Loss: 0.0010670949704945087\n",
      "Epoch: [459/500], Loss: 0.0\n",
      "Epoch: [460/500], Loss: 0.0\n",
      "Epoch: [461/500], Loss: 0.0014301433693617582\n",
      "Epoch: [462/500], Loss: 0.0\n",
      "Epoch: [463/500], Loss: 0.0010534743778407574\n",
      "Epoch: [464/500], Loss: 0.0002892968477681279\n",
      "Epoch: [465/500], Loss: 0.0\n",
      "Epoch: [466/500], Loss: 0.0006593288853764534\n",
      "Epoch: [467/500], Loss: 0.0\n",
      "Epoch: [468/500], Loss: 0.0005697936285287142\n",
      "Epoch: [469/500], Loss: 0.001382729271426797\n",
      "Epoch: [470/500], Loss: 0.0008331743883900344\n",
      "Epoch: [471/500], Loss: 0.0012727713910862803\n",
      "Epoch: [472/500], Loss: 0.0\n",
      "Epoch: [473/500], Loss: 0.0009083631448447704\n",
      "Epoch: [474/500], Loss: 0.0007650955813005567\n",
      "Epoch: [475/500], Loss: 0.0\n",
      "Epoch: [476/500], Loss: 0.0\n",
      "Epoch: [477/500], Loss: 0.0\n",
      "Epoch: [478/500], Loss: 0.0010320048313587904\n",
      "Epoch: [479/500], Loss: 0.0\n",
      "Epoch: [480/500], Loss: 0.0010636518709361553\n",
      "Epoch: [481/500], Loss: 0.0\n",
      "Epoch: [482/500], Loss: 0.0005632445681840181\n",
      "Epoch: [483/500], Loss: 0.0007127582794055343\n",
      "Epoch: [484/500], Loss: 0.0007550972513854504\n",
      "Epoch: [485/500], Loss: 0.0\n",
      "Epoch: [486/500], Loss: 0.0010078613413497806\n",
      "Epoch: [487/500], Loss: 0.0\n",
      "Epoch: [488/500], Loss: 0.0\n",
      "Epoch: [489/500], Loss: 0.000825548660941422\n",
      "Epoch: [490/500], Loss: 0.00044814523425884545\n",
      "Epoch: [491/500], Loss: 0.0\n",
      "Epoch: [492/500], Loss: 0.00039639254100620747\n",
      "Epoch: [493/500], Loss: 0.0\n",
      "Epoch: [494/500], Loss: 0.0003227741690352559\n",
      "Epoch: [495/500], Loss: 0.0006030140211805701\n",
      "Epoch: [496/500], Loss: 0.0006636743200942874\n",
      "Epoch: [497/500], Loss: 0.0\n",
      "Epoch: [498/500], Loss: 0.0\n",
      "Epoch: [499/500], Loss: 0.0005450852913782\n",
      "Epoch: [500/500], Loss: 0.0005725164664909244\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: [{epoch+1}/{epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader_val:\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, labels = next(iter(dataloader_val))\n",
    "# outputs = model(inputs.float())\n",
    "# _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "# for i, axes in enumerate(ax):\n",
    "#     axes.imshow(inputs[i].view(2, 5))\n",
    "#     axes.set_title(f'Predicted: {predicted[i]}, Actual: {labels[i]}')\n",
    "#     axes.axis('off')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
