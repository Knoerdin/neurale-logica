{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Imports and Variables](#import-variable)\n",
    "    1. [Imports](#imports)\n",
    "    2. [Set Variables](#variables)\n",
    "2. [Tokenizer](#tokenizer)\n",
    "    1. [Tokenizer Code *](#tokenizer-code)\n",
    "    2. [Create/Load Tokenizer *](#tokenizer-load)\n",
    "3. [Logic](#logic)\n",
    "    1. [Nodes](#nodes)\n",
    "    2. [Generate Formula](#generate-formula)\n",
    "    3. [Generate Premise](#generate-premise)\n",
    "    4. [Generate Key](#generate-key)\n",
    "    5. [Create Lookup Table](#lookup)\n",
    "    6. [Translate from Scentence (needs to be created) *](#to-formula)\n",
    "4. [Model](#model)\n",
    "    1. [Embedding and FeedForward](#embedding)\n",
    "    2. [Attention](#attention)\n",
    "    3. [Encoder](#encoder)\n",
    "    4. [Decoder](#decoder)\n",
    "    5. [Transformer](#transformer)\n",
    "5. [Dataset Generation](#generate-datasets)\n",
    "    1. [Dataset for Simple Models](#dataset-simple)\n",
    "6. [Training and Testing](#training-testen)\n",
    "    1. [Get Batch](#get-batch)\n",
    "    2. [Code for Training](#train-code)\n",
    "    3. [Code for Testing](#test-code)\n",
    "    4. [Model 1](#model1)\n",
    "\n",
    "Need to create model 2, training for model 2 and modify everything with * for model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import-variable\"></a>\n",
    "## Imports and Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"variables\"></a>\n",
    "### Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "batch_size = 512\n",
    "input_window_simple = 100\n",
    "input_window_complex = 200\n",
    "dataset_size = 100000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenizer\"></a>\n",
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenizer-code\"></a>\n",
    "### Tokenizer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "\n",
    "    @staticmethod\n",
    "    def create_vocab(tokens):\n",
    "        vocab = dict()\n",
    "        vocab['<PAD>'] = len(vocab)\n",
    "        vocab['<UNK>'] = len(vocab)\n",
    "        vocab['<PRED>'] = len(vocab)\n",
    "        vocab['<END>'] = len(vocab)\n",
    "        for token in tokens: vocab[str(token)] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def __init__(self, vocab, scentences= False):\n",
    "        self.encoder = vocab\n",
    "        self.scentences = scentences\n",
    "        self.decoder = {index: str(token) for token, index in vocab.items()}\n",
    "\n",
    "    def __len__(self): return len(self.encoder)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if self.scentences: return self.tokenize_sentence(text)\n",
    "        else: return self.tokenize_formula(text)\n",
    "\n",
    "    def tokenize_formula(self, text):\n",
    "        split = list(text.replace(' ', ''))\n",
    "        output = []\n",
    "        index = 0\n",
    "        while index < len(split):\n",
    "            if split[index] == '<':\n",
    "                output += [''.join(split[index:index+3])]\n",
    "                index += 3\n",
    "            elif split[index] == 'p' or split[index] == '-':\n",
    "                output += [''.join(split[index:index+2])]\n",
    "                index += 2\n",
    "            else:\n",
    "                output += [split[index]]\n",
    "                index += 1\n",
    "        return output\n",
    "    \n",
    "    def tokenize_sentence(self, text):\n",
    "        words = text.split(' ')\n",
    "        output = []\n",
    "\n",
    "    def encode(self, tokens): return [self.encoder[token] if token in self.encoder else self.encoder['<UNK>'] for token in tokens] \n",
    "\n",
    "    def decode(self, indices): return [self.decoder[index] if index in self.decoder else '<UNK>' for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenizer-load\"></a>\n",
    "### Create/Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scentence_tokenizer():\n",
    "    return Tokenizer(Tokenizer.create_vocab(['temp']), scentences=True)\n",
    "\n",
    "def create_formula_tokenizer():\n",
    "    return Tokenizer(Tokenizer.create_vocab(['(', ')', ',', '|', '&', '->', '<->', '!', 'p0', 'p1', 'p2', 'p3', '.', '{', '}']), scentences=False)\n",
    "\n",
    "formula_tokenizer = create_formula_tokenizer()\n",
    "scentence_tokenizer = create_scentence_tokenizer()\n",
    "PAD_ID = torch.tensor(formula_tokenizer.encoder['<PAD>']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logic\"></a>\n",
    "## Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nodecode\"></a>\n",
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def evaluate(self, values):\n",
    "        pass\n",
    "\n",
    "    def to_scentence(self):\n",
    "        pass\n",
    "\n",
    "    def get_depth(self):\n",
    "        pass\n",
    "\n",
    "    def get_atoms(self):\n",
    "        pass\n",
    "\n",
    "    def parse(text):\n",
    "        text = text.replace(' ', '')\n",
    "        if text[0] == '(' and text[-1] == ')':\n",
    "            text = text[1:-1]\n",
    "\n",
    "        if '(' not in text:\n",
    "            if '|' in text:\n",
    "                return Disjunction(Node.parse(text[:text.index('|')]), Node.parse(text[text.index('|') + 1:]))\n",
    "            elif '&' in text:\n",
    "                return Conjunction(Node.parse(text[:text.index('&')]), Node.parse(text[text.index('&') + 1:]))\n",
    "            elif '<->' in text:\n",
    "                return Biimplication(Node.parse(text[:text.index('<->')]), Node.parse(text[text.index('<->') + 3:]))\n",
    "            elif '->' in text:\n",
    "                return Implication(Node.parse(text[:text.index('->')]), Node.parse(text[text.index('->') + 2:]))\n",
    "            elif '!' in text:\n",
    "                return Negation(Node.parse(text[1:]))\n",
    "            return Variable(text, int(text[1:]))\n",
    "\n",
    "        depth = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] == '(':\n",
    "                depth += 1\n",
    "            elif text[i] == ')':\n",
    "                depth -= 1\n",
    "            elif depth == 0 and text[i] == '|':\n",
    "                return Disjunction(Node.parse(text[:i]), Node.parse(text[i + 1:]))\n",
    "            elif depth == 0 and text[i] == '&':\n",
    "                return Conjunction(Node.parse(text[:i]), Node.parse(text[i + 1:]))\n",
    "            elif depth == 0 and text[i] == '-' and text[i + 1] == '>':\n",
    "                return Implication(Node.parse(text[:i]), Node.parse(text[i + 2:]))\n",
    "            elif depth == 0 and text[i] == '<' and text[i + 1] == '-' and text[i + 2] == '>':\n",
    "                return Biimplication(Node.parse(text[:i]), Node.parse(text[i + 3:]))\n",
    "            elif depth == 0 and text[i] == '!':\n",
    "                return Negation(Node.parse(text[i + 1:]))\n",
    "        \n",
    "        raise Exception('Something went wrong')\n",
    "    \n",
    "    def __str__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "class Variable(Node):\n",
    "    def __init__(self, name, index):\n",
    "        self.name = name\n",
    "        self.index = index\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return values[self.index]\n",
    "    \n",
    "    def to_scentence(self, root = True):\n",
    "        return f'{self.name}{'.' if root else ''}', 0\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return [self.index]\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Variable):\n",
    "            return self.name == other.name\n",
    "        if isinstance(other, Negation):\n",
    "            return self.name == other.expr.name\n",
    "        return False\n",
    "\n",
    "class Negation(Node):\n",
    "    def __init__(self, expr):\n",
    "        self.expr = expr\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return not self.expr.evaluate(values)\n",
    "    \n",
    "    def to_scentence(self, root = True):\n",
    "        text, depth = self.expr.to_scentence(root = False)\n",
    "        return f'!{text}{'.' if root else ''}', depth\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return list({-x for x in self.expr.get_atoms()})\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return self.expr.get_depth()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"!{self.expr}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.expr)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Negation):\n",
    "            return self.expr == other.expr\n",
    "        if isinstance(other, Variable):\n",
    "            return self.expr.name == other.name\n",
    "        return False\n",
    "\n",
    "class Implication(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return not self.left.evaluate(values) or self.right.evaluate(values)\n",
    "    \n",
    "    def to_scentence(self, nested = False, root = True):\n",
    "        left_text, left_depth = self.left.to_scentence(nested=True, root=False) if isinstance(self.left, Implication) else self.left.to_scentence(root=False)\n",
    "        right_text, right_depth = self.right.to_scentence(root=False)\n",
    "\n",
    "        depth = max(left_depth, right_depth)\n",
    "        return f'{'if ' if not nested else ''}{left_text}{',' * depth} then {right_text}{'.' if root else ''}', depth + 1\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return list({x for x in self.left.get_atoms() + self.right.get_atoms()})\n",
    "\n",
    "    def get_depth(self):\n",
    "        return max(self.left.get_depth(), self.right.get_depth()) + 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.left} -> {self.right})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left) + len(self.right)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Implication):\n",
    "            return self.left == other.left and self.right == other.right\n",
    "        return False\n",
    "\n",
    "class Disjunction(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return self.left.evaluate(values) or self.right.evaluate(values)\n",
    "    \n",
    "    def to_scentence(self, root = True):\n",
    "        left_text, left_depth = self.left.to_scentence(root=False)\n",
    "        right_text, right_depth = self.right.to_scentence(root=False)\n",
    "        depth = max(left_depth, right_depth)\n",
    "        return f'{left_text}{',' * depth} or {right_text}{'.' if root else ''}', depth + 1\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return list({x for x in self.left.get_atoms() + self.right.get_atoms()})\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return max(self.left.get_depth(), self.right.get_depth()) + 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.left} | {self.right})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left) + len(self.right)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Disjunction):\n",
    "            return self.left == other.left and self.right == other.right\n",
    "        return False   \n",
    "\n",
    "class Conjunction(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return self.left.evaluate(values) and self.right.evaluate(values)\n",
    "    \n",
    "    def to_scentence(self, root = True):\n",
    "        left_text, left_depth = self.left.to_scentence(root=False)\n",
    "        right_text, right_depth = self.right.to_scentence(root=False)\n",
    "        depth = max(left_depth, right_depth)\n",
    "        return f'{left_text}{',' * depth} and {right_text}{'.' if root else ''}', depth + 1\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return list({x for x in self.left.get_atoms() + self.right.get_atoms()})\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return max(self.left.get_depth(), self.right.get_depth()) + 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.left} & {self.right})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left) + len(self.right)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Conjunction):\n",
    "            return self.left == other.left and self.right == other.right\n",
    "        return False\n",
    "    \n",
    "class Biimplication(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        return self.left.evaluate(values) == self.right.evaluate(values)\n",
    "\n",
    "    def to_scentence(self, root = True):\n",
    "        left_text, left_depth = self.left.to_scentence(root=False)\n",
    "        right_text, right_depth = self.right.to_scentence(nested = True, root=False) if isinstance(self.right, Implication) else self.right.to_scentence(root=False)\n",
    "        depth = max(left_depth, right_depth)\n",
    "        return f'{left_text} if{',' * depth} and only if {right_text}{'.' if root else ''}', depth + 1\n",
    "    \n",
    "    def get_atoms(self):\n",
    "        return list({x for x in self.left.get_atoms() + self.right.get_atoms()})\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return max(self.left.get_depth(), self.right.get_depth()) + 1\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"({self.left} <-> {self.right})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left) + len(self.right)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Biimplication):\n",
    "            return self.left == other.left and self.right == other.right\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate-formula\"></a>\n",
    "### Generate Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_formula(max, n):\n",
    "    output = []\n",
    "    for _ in range(max + 1):\n",
    "        output.append([])\n",
    "\n",
    "    for option in generate(max, n):\n",
    "        index = option.get_depth()\n",
    "        output[index].append(option)\n",
    "    return output\n",
    "\n",
    "def generate(depth, n):\n",
    "    for i in range(n):\n",
    "        yield Variable(f\"p{i}\", i)\n",
    "\n",
    "    if depth == 0:\n",
    "        for neg in generate(depth - 1, n):\n",
    "            yield Negation(neg)\n",
    "    elif depth > 0:\n",
    "        for left in generate(depth - 1, n):\n",
    "            for right in generate(depth - 1, n):\n",
    "                if left != right:\n",
    "                    yield Implication(left, right)\n",
    "                    yield Disjunction(left, right)\n",
    "                    yield Conjunction(left, right)\n",
    "                    yield Biimplication(left, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate-premise\"></a>\n",
    "### Generate Premise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_premises(key, odds, unqiue_keys):\n",
    "    options = list(split_formula(key, unqiue_keys))\n",
    "    if len(options) == 0: return None\n",
    "    np.random.shuffle(options)\n",
    "\n",
    "    if len(odds) == 0 or np.random.rand() < odds[0]:\n",
    "        return options[0]\n",
    "\n",
    "    for option in options:\n",
    "        splits_1 = generate_premises(option[0], odds[1:], unqiue_keys)\n",
    "        splits_2 = generate_premises(option[1], odds[1:], unqiue_keys)\n",
    "\n",
    "        if splits_1 is not None and splits_2 is not None:\n",
    "            if np.random.rand() < 0.5:\n",
    "                return (*splits_1, option[1])\n",
    "            else:\n",
    "                return (option[0], *splits_2)\n",
    "        elif splits_1 is not None:\n",
    "            return (*splits_1, option[1])\n",
    "        elif splits_2 is not None:\n",
    "            return (option[0], *splits_2)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def split_formula(value, indexes):\n",
    "    for i in indexes:\n",
    "        if i < value: continue\n",
    "        for j in indexes:\n",
    "            if j < value: continue\n",
    "            if i != value and j != value and i > j and i & j == value:\n",
    "                yield i, j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate-key\"></a>\n",
    "### Generate Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_number(expression, n):\n",
    "    output = 0\n",
    "    for i in range(2 ** n):\n",
    "        values = [bool(i & (1 << j)) for j in range(n)]\n",
    "        if expression.evaluate(values):\n",
    "            output += 2 ** i\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"to-formula\"></a>\n",
    "### Translate to Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_formula(expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lookup\"></a>\n",
    "#### Create Lookup Table for Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup():\n",
    "    table = dict()\n",
    "    table['by key'] = dict()\n",
    "    table['by length'] = dict()\n",
    "    table['by atoms'] = dict()\n",
    "    table['expressions'] = dict()\n",
    "\n",
    "    for formula in generate_formula(2, 4):\n",
    "        for expression in formula:\n",
    "            key = generate_number(expression, 4)\n",
    "            if key not in table['by key']:\n",
    "                table['by key'][key] = []\n",
    "            table['by key'][key] += [expression]\n",
    "\n",
    "            length = len(expression)\n",
    "            if length not in table['by length']:\n",
    "                table['by length'][length] = []\n",
    "            table['by length'][length] += [expression]\n",
    "\n",
    "            atoms = len(expression.get_atoms())\n",
    "            if atoms not in table['by atoms']:\n",
    "                table['by atoms'][atoms] = []\n",
    "            table['by atoms'][atoms] += [expression]\n",
    "            \n",
    "            table['expressions'][str(expression)] = dict()\n",
    "            table['expressions'][str(expression)]['key'] = key\n",
    "            table['expressions'][str(expression)]['length'] = length\n",
    "            table['expressions'][str(expression)]['atoms'] = atoms\n",
    "\n",
    "    return table\n",
    "\n",
    "lookup_table = create_lookup()\n",
    "simple_keys = list(lookup_table['by key'].keys())\n",
    "simple_lengths = list(lookup_table['by length'].keys())\n",
    "simple_atoms = list(lookup_table['by atoms'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"embedding\"></a>\n",
    "### Embedding and FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # self.embedding_pos = nn.Embedding(context_size, embedding_dim)\n",
    "\n",
    "        self.embedding_pos = torch.zeros(context_size, embedding_dim)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float()) * (-math.log(10000)/embedding_dim)\n",
    "        pos = torch.arange(0, context_size, dtype=torch.float).unsqueeze(1)\n",
    "        self.embedding_pos[:, 0::2] = torch.sin(pos * div_term)\n",
    "        self.embedding_pos[:, 1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        self.embedding_pos = self.embedding_pos.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input) * math.sqrt(self.embedding_dim)\n",
    "        return input + (self.embedding_pos[:, :input.shape[1], :]).requires_grad_(False) \n",
    "        #return self.embedding(input) + self.embedding_pos(input).to(device)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(ff_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.linear_1(input)\n",
    "        input = self.relu(input)\n",
    "        input = self.linear_2(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attention\"></a>\n",
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, head_dimension, context_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "\n",
    "        ones = torch.ones(size=[context_size, context_size], dtype=torch.float)\n",
    "        self.register_buffer(name=\"mask\", tensor=torch.tril(input=ones))\n",
    "    \n",
    "    def forward(self, input, masked = False):\n",
    "        B, T, C = input.size()\n",
    "\n",
    "        query = self.query(input)\n",
    "        key = self.key(input)\n",
    "        value = self.value(input)\n",
    "\n",
    "        qk = query @ key.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        attention = qk.masked_fill(self.mask[:T,:T] == 0, float(\"-inf\")) if masked else qk\n",
    "        attention = F.softmax(input=attention, dim=-1)\n",
    "\n",
    "        out = attention @ value\n",
    "        return out\n",
    "    \n",
    "class MultiAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, context_size, n_head):\n",
    "        super().__init__()\n",
    "        head_dim = embedding_dim // n_head\n",
    "        assert head_dim * n_head == embedding_dim, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([AttentionBlock(embedding_dim, head_dim, context_size) for _ in range(n_head)])\n",
    "        \n",
    "    def forward(self, query, key, value, masked = False):\n",
    "        B, T, C = query.size()\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        qk = query @ key.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        attention = qk\n",
    "        attention = F.softmax(input=attention, dim=-1)\n",
    "\n",
    "        out = attention @ value\n",
    "        return torch.cat([attention_block(out, masked) for attention_block in self.attention_blocks], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encoder\"></a>\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, context_size, embedding_dim, hidden_dim, n_head):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_attention = MultiAttentionBlock(embedding_dim, context_size, n_head)\n",
    "        self.feed_forward = FeedForward(embedding_dim, hidden_dim)\n",
    "        self.norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        value = self.multi_attention(input, input, input)\n",
    "        input = self.norm_1(input + value)\n",
    "        value = self.feed_forward(input)\n",
    "        return self.norm_2(input + value)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_dim, n_head, encoder_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, context_size, embedding_dim)\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(context_size, embedding_dim, hidden_dim, n_head) for _ in range(encoder_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input)\n",
    "        for encoder in self.encoders: input = encoder(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decoder\"></a>\n",
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_size, embedding_dim, hidden_dim, n_head):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_attention_masked = MultiAttentionBlock(embedding_dim, context_size, n_head)\n",
    "        self.multi_attention = MultiAttentionBlock(embedding_dim, context_size, n_head)\n",
    "        self.feed_forward = FeedForward(embedding_dim, hidden_dim)\n",
    "        self.norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm_3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        value = self.multi_attention_masked(decoder_input, decoder_input, decoder_input, masked=True)\n",
    "        decoder_input = self.norm_1(decoder_input + value)\n",
    "        value = self.multi_attention(encoder_input, encoder_input, decoder_input)\n",
    "        decoder_input = self.norm_2(decoder_input + value)\n",
    "        value = self.feed_forward(decoder_input)\n",
    "        return self.norm_3(decoder_input + value)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_dim, n_head, decoder_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, context_size, embedding_dim)\n",
    "        self.decoders = nn.ModuleList([DecoderLayer(context_size, embedding_dim, hidden_dim, n_head) for _ in range(decoder_layers)])\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        decoder_input = self.embedding(decoder_input)\n",
    "        for decoder in self.decoders: decoder_input = decoder(encoder_input, decoder_input)\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transformer\"></a>\n",
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_dim, n_head= 8, encoder_layers= 3, decoder_layers= 3):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, context_size, embedding_dim, hidden_dim, n_head, encoder_layers)\n",
    "        self.decoder = Decoder(vocab_size, context_size, embedding_dim, hidden_dim, n_head, decoder_layers)\n",
    "        self.projection = nn.Linear(context_size * embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_output = self.encoder(encoder_input)\n",
    "        decoder_output = self.decoder(encoder_output, decoder_input)\n",
    "        out = self.projection(decoder_output.view(decoder_output.size(0), -1))\n",
    "        return F.softmax(out, dim=-1)\n",
    "    \n",
    "    def predict(self, encoder_input, decoder_input, greedy = False):\n",
    "        with torch.no_grad():\n",
    "            if greedy: return torch.argmax(self(encoder_input, decoder_input), dim=-1)\n",
    "            return torch.multinomial(self(encoder_input, decoder_input)[-1], num_samples=1)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate-dataset\"></a>\n",
    "## Generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset-code\"></a>\n",
    "### Code to Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_dataset(tokenizer, size: int, atoms: int, depth, premise_odds: list):\n",
    "    output = []\n",
    "    \n",
    "    options = list(generate(depth, atoms))\n",
    "    counter = 0\n",
    "    np.random.shuffle(options)\n",
    "\n",
    "    for conclusion in options:\n",
    "        key = generate_number(conclusion, atoms)\n",
    "        splits = generate_premises(key, premise_odds, simple_keys)\n",
    "        if splits is None:\n",
    "            continue\n",
    "        counter += 1\n",
    "        new_premises = [np.random.choice(lookup_table['by key'][split]) for split in splits]\n",
    "        token_premises = [tokenizer.encode(tokenizer.tokenize(str(premise)) + ['.']) for premise in new_premises]\n",
    "        encoder_input = [tokenizer.encoder['{']]\n",
    "        for token_premise in token_premises:\n",
    "            encoder_input += token_premise\n",
    "        encoder_input += [tokenizer.encoder['}']]\n",
    "        encoder_input += [tokenizer.encoder['<PAD>']] * (input_window_simple - len(encoder_input))\n",
    "        \n",
    "        decoder_input = tokenizer.encode(tokenizer.tokenize(str(conclusion)) + ['<END>'])\n",
    "\n",
    "        for i in range(len(decoder_input)):\n",
    "            new_datapoint = []\n",
    "            new_datapoint.append(encoder_input)\n",
    "            new_datapoint.append(decoder_input[:i] + [tokenizer.encoder['<PRED>']] + [tokenizer.encoder['<PAD>']] * (input_window_simple - len(decoder_input[:i]) - 1))\n",
    "            new_datapoint.append(decoder_input[i])\n",
    "            output.append(new_datapoint)\n",
    "        if counter == size:\n",
    "            return output\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset-simple\"></a>\n",
    "### Generate Dataset for Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#simple_dataset = create_simple_dataset(formula_tokenizer, dataset_size, 4, 2, [0.5, 0.5, 0.5])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m complex_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_complex_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscentence_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mcreate_complex_dataset\u001b[0;34m(tokenizer, size, atoms, depth, premise_odds)\u001b[0m\n\u001b[1;32m     13\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m new_premises \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(lookup_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby key\u001b[39m\u001b[38;5;124m'\u001b[39m][split]) \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits]\n\u001b[0;32m---> 15\u001b[0m token_premises \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m premise \u001b[38;5;129;01min\u001b[39;00m new_premises]\n\u001b[1;32m     16\u001b[0m encoder_input \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_premise \u001b[38;5;129;01min\u001b[39;00m token_premises:\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscentences: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_formula(text)\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mTokenizer.tokenize_sentence\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 41\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "simple_dataset = create_simple_dataset(formula_tokenizer, dataset_size, 4, 2, [0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-testen\"></a>\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get-batch\"></a>\n",
    "#### Get Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size = 64):\n",
    "    while True:\n",
    "        np.random.shuffle(dataset)\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            encoder_input = []\n",
    "            decoder_input = []\n",
    "            target_labels = []\n",
    "            for j in range(i, min(i + batch_size, len(dataset))):\n",
    "                encoder_input.append(dataset[j][0])\n",
    "                decoder_input.append(dataset[j][1])\n",
    "                target_labels.append(dataset[j][2])\n",
    "            yield torch.tensor(encoder_input, dtype= torch.int64).to(device), torch.tensor(decoder_input, dtype= torch.int64).to(device), torch.tensor(target_labels, dtype= torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-code\"></a>\n",
    "#### Code for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataset, epochs, batch_size, save_path, save_interval = 1000, save=False):\n",
    "    batch = get_batch(dataset, batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        encoder_input, decoder_input, decoder_target = next(batch)\n",
    "\n",
    "        decoder_output = model(encoder_input, decoder_input)\n",
    "        loss = F.cross_entropy(decoder_output, decoder_target, ignore_index=PAD_ID)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 100 == 0 or epoch == epochs - 1:\n",
    "                print(f'Epoch {epoch + 1} Loss {loss.item()}')\n",
    "        if save and (epoch + 1) % save_interval == 0: torch.save(model, save_path)\n",
    "    \n",
    "    if save: torch.save(model, save_path)\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-code\"></a>\n",
    "#### Code for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_test(model, tokenizer, n_prob = 10, n_greedy = 10):\n",
    "    columns = ['Premises', 'Generated Conclusion', 'Greedy', 'Target', 'Generated Key', 'Valid', 'Correct']\n",
    "    results = []\n",
    "    for n in range(n_prob + n_greedy):\n",
    "        new_results = [0, 0, 1, 0, -1, 0, 0]\n",
    "        if n < n_prob: new_results[2] = 0\n",
    "\n",
    "\n",
    "        datapoint = np.random.choice(lookup_table['by atoms'][np.random.choice(simple_atoms)])\n",
    "        new_results[3] = lookup_table['expressions'][str(datapoint)]['key']\n",
    "\n",
    "        decoder_sequence = tokenizer.encode(['<PRED>'] + ['<PAD>'] * (input_window_simple - 1))\n",
    "\n",
    "        premises = generate_premises(lookup_table['expressions'][str(datapoint)]['key'], [0.5, 0.5, 0.5], simple_keys)\n",
    "        while premises is None:\n",
    "            datapoint = np.random.choice(lookup_table['by atoms'][np.random.choice(simple_atoms)])\n",
    "            premises = generate_premises(lookup_table['expressions'][str(datapoint)]['key'], [0.5, 0.5, 0.5], simple_keys)\n",
    "\n",
    "        encoder_sequence = tokenizer.encode(['{'])\n",
    "        new_results[0] = '{'\n",
    "\n",
    "        for premise in premises:\n",
    "            premise_formula = np.random.choice(lookup_table['by key'][premise])\n",
    "            encoder_sequence += tokenizer.encode(tokenizer.tokenize(str(premise_formula)) + ['.'])\n",
    "            new_results[0] += str(premise_formula) + '. '\n",
    "        new_results[0] += '}'\n",
    "\n",
    "        encoder_sequence += tokenizer.encode(['}'])\n",
    "        encoder_sequence += [tokenizer.encoder['<PAD>']] * (input_window_simple - len(encoder_sequence))\n",
    "        encoder_input = torch.tensor(encoder_sequence, dtype= torch.int64).reshape(1,-1).to(device)\n",
    "\n",
    "        for i in range(input_window_simple):\n",
    "            decoder_input = torch.tensor(decoder_sequence, dtype= torch.int64).reshape(1,-1).to(device)\n",
    "            \n",
    "            new_token = model.predict(encoder_input, decoder_input, greedy = (n >= n_prob))\n",
    "            if new_token.item() == tokenizer.encoder['<END>']:\n",
    "                break\n",
    "            \n",
    "            decoder_sequence[i] = new_token.item()\n",
    "            if i < input_window_simple - 1:\n",
    "                decoder_sequence[i + 1] = tokenizer.encoder['<PRED>']\n",
    "\n",
    "        output = ''.join(tokenizer.decode(decoder_sequence[:i]))\n",
    "        new_results[1] = output\n",
    "        try:\n",
    "            output = Node.parse(output)\n",
    "            new_results[4] = lookup_table['expressions'][str(output)]['key']\n",
    "            new_results[5] = 1\n",
    "            new_results[6] = 1 if lookup_table['expressions'][str(output)]['key'] == new_results[3] else 0\n",
    "        except:\n",
    "            pass\n",
    "        results.append(new_results)\n",
    "    return pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model1\"></a>\n",
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4392/3367201255.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1 = torch.load('new_model_1.pth')\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(vocab_size=len(formula_tokenizer), context_size=input_window_simple, embedding_dim=32, hidden_dim=1024, n_head=8, encoder_layers= 12, decoder_layers= 12).to(device)\n",
    "optimizer_1 = torch.optim.AdamW(model_1.parameters(), lr=learning_rate)\n",
    "model_1 = train(model_1, optimizer_1, simple_dataset, epochs, batch_size, save_path='new_model_1.pth', save=True, save_interval=1000)\n",
    "#model_1 = torch.load('new_model_1.pth')\n",
    "#print(sum(p.numel() for p in model_1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test(model_1, formula_tokenizer, n_prob=1000, n_greedy=1000).to_csv('simple_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
